---
title: Autonomous Fine-Tuning
icon: robot
---
The Gym is a powerful feature that allows developers to **autonomously fine-tune** AI models without any human intervention.

Unlike traditional fine-tuning approaches that require extensive manually curated datasets and considerable human oversight, 
the Gym maximizes model performance through sophisticated data augmentation and distributed training architectures.


<Tip>
Want some more information on how the Gym works under the hood? 
Click [here](#under-the-hood) to learn how the autonomous fine-tuning agent works under the hood.
</Tip>



# Best Practices

### **Data Preparation: Start Small, Iterate Frequently**.
  - **Begin with ~50 well-curated examples** for your specific use case. Even a small dataset can serve as a strong foundation when combined with the system‚Äôs autonomous data augmentation.
  - **Conduct short training runs** to quickly evaluate if you‚Äôre on the right track. Shorter, frequent training cycles let you detect issues early‚Äîsuch as mislabeled data or unsuitable hyperparameters‚Äîwithout expending excessive resources.

### **Use the Prem Playground for Rapid Feedback**
    - **Leverage the interactive comparison feature** in the Prem Playground to quickly gauge which model variant is performing well on your task.


    - **Perform red-teaming tests** by providing diverse, challenging prompts. This not only uncovers vulnerabilities but also creates high-value data for subsequent fine-tuning stages.

### **Emphasize Quality in Data Collection**

    - **Automated data harvesting** from production systems is powerful, but ensure you incorporate clear filtering and quality checks. Use domain-specific rules and engagement metrics to keep your training data relevant and noise-free.

    - **Multi-dimensional annotations** (accuracy, relevance, style, etc.) can provide richer signals for model training, resulting in more robust outcomes.

### **Make the Most of Autonomous Data Augmentation**

    - **Rely on the platform‚Äôs multi-agent orchestration** to expand your dataset systematically without compromising on semantic consistency.

    - **Configure domain constraints** carefully to avoid generating irrelevant or contradictory samples. For instance, if you‚Äôre focusing on code generation, ensure that your sandboxed environment and validation criteria are well-defined before augmentation begins.

### **Choose the Right Model Family & Resources**


    - **Select a model family** that aligns with your task domain (NL2NL, NL2C, or specialized models like Text to SQL). Each family has different baseline capabilities and constraints.
    - **Budget resources** based on your expected training duration and hardware availability. The system‚Äôs distributed training and dynamic resource allocation work best when you provide realistic constraints upfront.

### **Continuously Evaluate & Prune**

    - **Run parallel trainings** on different model candidates and let the system‚Äôs leaderboard identify top performers. This multi-model strategy helps surface unexpected winners (smaller models may outperform bigger ones for certain tasks).


    - **Prune underperforming models** early to save computational resources. Focus on refining promising candidates with additional synthetic data or domain-specific fine-tuning.

### **Close the Loop with Active Learning**

    - **Continually gather new examples** from both user feedback and system logs. The more ‚Äúreal-world‚Äù data you incorporate, the more resilient your models become.
    - **Use feedback modification tools** in the Prem Platform to annotate or correct model responses. These corrected examples directly fuel the next iteration of fine-tuning.


### **Integrate via API for Automation**

    - **Automate your data pipelines** by calling Prem‚Äôs fine-tuning APIs, enabling you to trigger new training jobs as soon as fresh data becomes available.
    - **Implement continuous integration (CI) checks** that automatically spin up small-scale training runs for any new dataset changes, ensuring quality remains consistent over time.


# Step by Step Guide

<Steps>
    <Step title="Gather your data">

    In order to fine-tune your model, you are required to submit at least **50** well-curated datapoints/positive feedback responses.
    
    Chat with your model of choice in the [Lab](/get-started/lab) or through the [SDK](/get-started/sdk)/[API](/api-reference/introduction). This allows you to gather positive feedback responses whether its during development or in production.
        
    </Step>
    <Step title="Provide Feedback">
        <Tip>
         In order to fine-tune your model, you are required to submit at least **50** postive feedback responses. 
         Just click the thumbs up üëç for postive feedback and thumbs down üëé for negative feedback.  
        </Tip>

  <img
  src="https://static.premai.io/prem-saas-docs/gym-guide/finetuneTraces.gif"
  alt="Playground"
/>
<Tip>
We suggest that you use the **Traces** sections to submit feedback. 


Alterntively, you can also provide feedback from the [Lab](/get-started/lab) shown below.


</Tip>
 
 <img
  src="https://static.premai.io/prem-saas-docs/gym-guide/labPositiveFeedback.gif"
  alt="Playground"
/>
        
    </Step>
      <Step title="Choose a model and start fine-tuning">
      Simply click the **_Start fine-tuning_** button to begin fine tuning.
  <img
  src="https://static.premai.io/prem-saas-docs/gym-guide/initateFinetune.png"
  alt="Playground"
/>
  You'll see whats represented in the image below once you initiate the fine-tuning.
  <img
  src="https://static.premai.io/prem-saas-docs/gym-guide/tuning.png"
  alt="Playground"
/>
    </Step>
      <Step title="You'll recieve confirmation emails">

      The first email will confirm that the fine-tuning process has begun.
          <img
  src="https://static.premai.io/prem-saas-docs/gym-guide/fintuneStartEmail.png"
  alt="Playground"
/>
The second email will confirm that the fine-tuning has been completed.

          <img
  src="https://static.premai.io/prem-saas-docs/gym-guide/finetuneEndEmail.png"
  alt="Playground"
/>
    </Step>
    <Step title="Try your fine-tuned model in the lab">
        Now that the fine-tuning is complete, head over to the **Lab** to test it out. You can search for your fine-tuned model the same way you would with any of the pre-trained models available. You can always change the system prompt and fine-tune again until you get your model working the way you want it to.

        <Tip>
        You can also test out your fine-tuned model indivdually in the chat section of the **Lab**

         <img
  src="https://static.premai.io/prem-saas-docs/gym-guide/finetunedChat.png"
  alt="Playground"
/>
        </Tip>
    </Step>

    <Step title="Launch your fine-tuned model with the Launchpad">
        Now that the fine-tuning and testing are complete, navigate to the **Launchpad** to deploy your model so it's ready for integrating into your applications. You will still have the ability to configure the model's params, system prompts and repositories.
             <img
  src="https://static.premai.io/prem-saas-docs/gym-guide/zoomedfinetunedLaunch.png"
  alt="Playground"
/>
    </Step>
</Steps>

# Under the Hood
